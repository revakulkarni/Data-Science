{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52ad1b17",
   "metadata": {},
   "source": [
    "# Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc7fee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'months']\n",
      "['Hello I am Gayatri Deshmukh.', 'I am from Nanded District.', 'I will be an Engineer in few months']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Reva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Reva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#tokenization = breaking a sentence into words\n",
    "# NLTK = Natural language ToolKit\n",
    "#punkt is a tokenizer\n",
    "#wordnet is a database for english library\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "sentence = \"Hello I am Gayatri Deshmukh. I am from Nanded District. I will be an Engineer in few months \"\n",
    "\n",
    "tokenized_words = nltk.word_tokenize(sentence)\n",
    "tokenized_sentences = nltk.sent_tokenize(sentence)\n",
    "\n",
    "print(tokenized_words)\n",
    "print(tokenized_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db50370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Reva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('Gayatri', 'NNP'),\n",
       " ('Deshmukh', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('from', 'IN'),\n",
       " ('Nanded', 'NNP'),\n",
       " ('District', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('I', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('be', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('Engineer', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('few', 'JJ'),\n",
       " ('months', 'NNS')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#POS Tagging =Part of speech tagging\n",
    "#averaged_perceptron_tagger= ml algo in ntlk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "postagging=nltk.pos_tag(tokenized_words)\n",
    "postagging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "053c155d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unclean version  ['Hello', 'I', 'am', 'Gayatri', 'Deshmukh', '.', 'I', 'am', 'from', 'Nanded', 'District', '.', 'I', 'will', 'be', 'an', 'Engineer', 'in', 'few', 'months']\n",
      "Clean Version ['Hello', 'I', 'Gayatri', 'Deshmukh', '.', 'I', 'Nanded', 'District', '.', 'I', 'Engineer', 'months']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Reva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#stopwords removal\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "cleaned_token = []\n",
    "\n",
    "for i in tokenized_words:\n",
    "    if i not in stop_words: \n",
    "        cleaned_token.append(i)\n",
    "\n",
    "print(\"Unclean version \", tokenized_words)\n",
    "print(\"Clean Version\", cleaned_token)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "65efefb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'i',\n",
       " 'am',\n",
       " 'gayatri',\n",
       " 'deshmukh',\n",
       " '.',\n",
       " 'i',\n",
       " 'am',\n",
       " 'from',\n",
       " 'nand',\n",
       " 'district',\n",
       " '.',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'an',\n",
       " 'engin',\n",
       " 'in',\n",
       " 'few',\n",
       " 'month']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemming = reducing words in their base / root form.......eg: males = male, roots = root, likes = like.\n",
    "from nltk.stem import SnowballStemmer\n",
    "snowball_stemmer=SnowballStemmer('english')\n",
    "stemmed_words=[]\n",
    "for i in tokenized_words:\n",
    "    stemmed=snowball_stemmer.stem(i)\n",
    "    stemmed_words.append(stemmed)\n",
    "stemmed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67bda84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Reva\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'I',\n",
       " 'am',\n",
       " 'Gayatri',\n",
       " 'Deshmukh',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'from',\n",
       " 'Nanded',\n",
       " 'District',\n",
       " '.',\n",
       " 'I',\n",
       " 'will',\n",
       " 'be',\n",
       " 'an',\n",
       " 'Engineer',\n",
       " 'in',\n",
       " 'few',\n",
       " 'month']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatization = similar to stemming but has some meaning\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "wordnet_lemma=WordNetLemmatizer()\n",
    "lemma=[]\n",
    "for i in tokenized_words:\n",
    "    lemmatized=wordnet_lemma.lemmatize(i)\n",
    "    lemma.append(lemmatized)\n",
    "\n",
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf vfid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d76301d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Indexing:  {'good': 4, 'morning': 8, 'do': 1, 'daily': 0, 'exercise': 2, 'in': 6, 'the': 9, 'is': 7, 'for': 3, 'health': 5}\n",
      "tf-idf values :: \n",
      "   (0, 8)\t0.7071067811865476\n",
      "  (0, 4)\t0.7071067811865476\n",
      "  (1, 9)\t0.4403620672313486\n",
      "  (1, 6)\t0.4403620672313486\n",
      "  (1, 2)\t0.3349067026613031\n",
      "  (1, 0)\t0.4403620672313486\n",
      "  (1, 1)\t0.4403620672313486\n",
      "  (1, 8)\t0.3349067026613031\n",
      "  (2, 5)\t0.49047908420610337\n",
      "  (2, 3)\t0.49047908420610337\n",
      "  (2, 7)\t0.49047908420610337\n",
      "  (2, 2)\t0.3730219858594306\n",
      "  (2, 4)\t0.3730219858594306\n",
      "tf-idf in matrix form: \n",
      " [[0.         0.         0.         0.         0.70710678 0.\n",
      "  0.         0.         0.70710678 0.        ]\n",
      " [0.44036207 0.44036207 0.3349067  0.         0.         0.\n",
      "  0.44036207 0.         0.3349067  0.44036207]\n",
      " [0.         0.         0.37302199 0.49047908 0.37302199 0.49047908\n",
      "  0.         0.49047908 0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "d0 = \"Good Morning\"\n",
    "d1 = \"Do daily exercise in the morning \"\n",
    "d2 = \"exercise is good for health\"\n",
    "series = [d0, d1, d2]\n",
    "tfidf = TfidfVectorizer()\n",
    "result = tfidf.fit_transform(series)\n",
    "\n",
    "print(\"Word Indexing: \", tfidf.vocabulary_)\n",
    "print(\"tf-idf values :: \\n\", result)\n",
    "print(\"tf-idf in matrix form: \\n\", result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa7c6b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
